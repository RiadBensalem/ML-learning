{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 02 : Régression linéaire \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Régression linéaire à une seule variable \n",
    "Dans cette partie, on commence par implémenter la régression linéaire avec une seule variable de prédiction (predictor). Nous allons donc essayer de résoudre le fameux problème de prédiction du prix d'une maison en connaissant sa superficie. \n",
    "\n",
    "### 1- Préparation des données : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = [\"Superficie\", \"Prix\"]\n",
    "houses = pd.read_csv(\"datasets/houses.csv\", names=header)\n",
    "houses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houses.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction des features \n",
    "X = houses.iloc[:, :-1].values # Premières colonnes \n",
    "\n",
    "Y = houses.iloc[:,-1].values # Dernière colonne "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalisation \n",
    "La normalisation est la mise à echelle des valeurs des caractéristiques. Exemple simple de but : En calculant la distance euclidienne une des caracteristiques va avoir plus d'effet sur le résultat si ses valeurs sont beaucoup plus grandes que celle de l'autre variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation \n",
    "X = None\n",
    "\n",
    "## On définit la matrice X comme etant la supérficie concatenée à un vecteur de 1 pour faciliter l'algorithme pour theta0 \n",
    "ones = np.ones([X.shape[0],1])\n",
    "X = np.concatenate((ones,X),axis=1)\n",
    "X[:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,1], Y)\n",
    "plt.xlabel('Superficie')\n",
    "plt.ylabel('Prix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Descente du gradient : \n",
    "\n",
    "#### 2.1 : Définir les hyperparamétres : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir les hyperparamétres : \n",
    "learning_rate = 0.01 \n",
    "nb_iter = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Définir la fonction de cout : \n",
    "def J(X,y,theta):\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 : Définir l'algorithme de la régression linéaire avec descente du gradient : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO :  Initialiser aléatoirement les paramètres :\n",
    "## theta est une liste contenant les deux paramètres theta0 et theta1 \n",
    "theta = None \n",
    "print(\"Thetas aléatoires : \", theta)\n",
    "\n",
    "\n",
    "M = len(X)\n",
    "\n",
    "# Affichage de la droite aléatoire \n",
    "plt.scatter(X[:,1], Y)\n",
    "plt.plot(X[:,1], np.dot(X, theta))\n",
    "plt.show()\n",
    "\n",
    "# TODO : Définir la déscente du gradient\n",
    "def gradient_descent(X, Y, theta, nb_iter, learning_rate, affich = 0):\n",
    "    evolution = nb_iter // 3 # Afficher les droites à chaque tiers \n",
    "    \n",
    "    # Définir l'algorithme de la descente de gradient : \n",
    "    \n",
    "        \n",
    "    # Affichage finale \n",
    "    plt.scatter(X[:,1], Y)\n",
    "    plt.plot(X[:,1], np.dot(X, theta))\n",
    "    plt.show()\n",
    "\n",
    "    return theta, cout_precedent\n",
    "\n",
    "theta_optimaux, couts_precedents = gradient_descent(X,Y, theta, nb_iter, learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Thétas optimaux : \", theta_optimaux)\n",
    "print(couts_precedents[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(couts_precedents)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1 ** : \n",
    "\n",
    "Pour cette exemple, quel est selon vous le nombre d'itérations nécessaires pour obtenir la convergence dans l'algorithme de la descente du gradient ? \n",
    "\n",
    "**Answer 1 : ** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2 ** : \n",
    "\n",
    "Essayer de changer les valeurs du learning_rate, que remarquez-vous ? \n",
    "Afficher le graphe des coûts (J) par rapport aux différents learning_rate. \n",
    "\n",
    "**Answer 2 : ** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage du graphe des coûts par rapport aux learning_rate :\n",
    "couts = []\n",
    "\n",
    "# TODO : Tester plusieurs valeurs du learning_rate : \n",
    "\n",
    "\n",
    "plt.plot(couts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation des paramétres : \n",
    "Initialisation différentes des thetas pour tester l'algorithme : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation aléatoire \n",
    "theta = None\n",
    "affich = 1 \n",
    "theta_optimaux, couts_precedents = gradient_descent(X,Y, theta, nb_iter, learning_rate, affich)\n",
    "\n",
    "print(couts_precedents[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation à zero \n",
    "theta = None\n",
    "affich = 1 \n",
    "theta_optimaux, couts_precedents = gradient_descent(X,Y, theta, nb_iter, learning_rate, affich)\n",
    "\n",
    "print(couts_precedents[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation à un \n",
    "theta = None\n",
    "affich = 1 \n",
    "theta_optimaux, couts_precedents = gradient_descent(X,Y, theta, nb_iter, learning_rate, affich)\n",
    "\n",
    "print(couts_precedents[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skin Cancer Datasets : \n",
    "Pour tester des données dans une forme différente et s'assurer que la ligne épouse au mieux les données nous allons exécuter l'algorithme sur les données du fichier skincancer.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header= [ \"Latitude\", \"Mortality\", \"Ocean\", \"Long\"]\n",
    "skinCancer = pd.read_csv(\"datasets/skincancer.txt\",  names=header)\n",
    "skinCancer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(skinCancer.iloc[:,0], skinCancer.iloc[:,1])\n",
    "plt.xlabel('Latitude')\n",
    "plt.ylabel('Mortality')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = skinCancer.iloc[:,:1].values\n",
    "Y = skinCancer.iloc[:,1].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation \n",
    "X = None \n",
    "\n",
    "ones = np.ones([len(X),1])\n",
    "X = np.concatenate((ones,X),axis=1)\n",
    "X[:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.random.rand(X.shape[1])\n",
    "plt.scatter(X[:,1], Y)\n",
    "plt.plot(X[:,1], np.dot(X, theta))\n",
    "plt.show()\n",
    "\n",
    "theta_optimaux, couts_precedents = gradient_descent(X,Y, theta, nb_iter, learning_rate)\n",
    "\n",
    "print(couts_precedents[-1])\n",
    "print(theta_optimaux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Régression lineaire avec scikit-learn : \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On poursuit avec les données du skin cancer : \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diviser les données \n",
    "from sklearn.model_selection import train_test_split  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraîner le modèle \n",
    "from sklearn.linear_model import LinearRegression  \n",
    "regressor = LinearRegression()  \n",
    "regressor.fit(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(regressor.intercept_)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(regressor.coef_)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = [regressor.intercept_ , regressor.coef_[1]]\n",
    "plt.scatter(X[:,1], Y)\n",
    "plt.plot(X[:,1], np.dot(X, theta))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédire les valeurs du X_test \n",
    "y_pred = regressor.predict(X_test)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Calculer la matrice de confusion : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quelques metrics proposés par sklearn \n",
    "from sklearn import metrics  \n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
